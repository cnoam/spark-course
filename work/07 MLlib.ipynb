{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44ab91d4-60f9-4b79-b338-e0c2b363aef4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Using the Machine Learning library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32141e84-fff5-44b4-9b6e-7885a8f23af2",
   "metadata": {
    "tags": []
   },
   "source": [
    "Spark provides a library with ML functionality. The set of tools is ever expanding -- see the latest at https://spark.apache.org/docs/latest/ml-guide.html\n",
    "\n",
    "The library is implemented in Scala, and has python binding (i.e. calling from python to the API).\n",
    "\n",
    "\n",
    "Using MLFlow ( https://mlflow.org/docs/latest/python_api/mlflow.spark.html?highlight=spark#module-mlflow.spark )is also possible, but not covered here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29449b8e-51c2-4d09-920b-4e5e54eb6efd",
   "metadata": {},
   "source": [
    "**Check the notebook at \"SDG/Advanced_Analytics_and_Machine_Learning-Chapter_25_Preprocessing_and_Feature_Engineering\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0239539-8e23-4526-a01a-6c740ce015a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "If a specific tool is not part of MLlib, maybe someone already implemented it.\n",
    "\n",
    "Always be suspicious of the source: who wrote it? when was the last update? how many stars?\n",
    "\n",
    "See for example https://spark-packages.org/?q=tags%3A%22Machine%20Learning%22 which is a repo without any quality assurance. You can find a great code, a buggy code, or malware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28feace4-9910-4b0c-b691-79daf4a55834",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/02/23 15:05:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import *\n",
    "spark = SparkSession.builder.appName('MLlib').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "181ac6fc-20ff-4f4d-9b04-0b7a47fb6cec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading ../data/sdg/retail-data/by-day/2011-_.parquet from cache Parquet file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.count = 499428\n"
     ]
    }
   ],
   "source": [
    "def load_data(file_name_glob):\n",
    "    \"\"\" load the contents of the input files.\n",
    "        If we already saved them in Parquet file, use it.\n",
    "        >>> load_data('../data/sdg/retail-data/by-day/2010-12*.csv')\n",
    "        :param file_name_glob wildcard value of the files to read. e.g. \"/mnt/dir/data*\"\n",
    "        :return: DataFrame containing all the data\n",
    "    \"\"\"\n",
    "    \n",
    "    def cache_file_name(file_name):\n",
    "        t = file_name.replace('*',\"_\").replace('?',\"_\")\n",
    "        return t[: t.rfind('.')] + \".parquet\"\n",
    "    \n",
    "    import os\n",
    "    dirname = os.path.dirname(file_name_glob)\n",
    "    p = Path(dirname)\n",
    "    fname = Path(file_name_glob)\n",
    "    basename = fname.name\n",
    "    cache_name = cache_file_name(file_name_glob)\n",
    "    if Path(cache_name).exists():\n",
    "        print(f\"reading {cache_name} from cache Parquet file\")\n",
    "        return spark.read.parquet(cache_name)\n",
    "    \n",
    "    #suffix = fname.suffix\n",
    "    if not p.exists():\n",
    "        raise ValueError('Path not found')\n",
    "    file_list = list(p.glob(basename))\n",
    "    x = [ str(f.resolve()) for f in file_list]\n",
    "    df = spark.read \\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .csv(x)\n",
    "    \n",
    "    df.write.parquet(cache_name)\n",
    "    return df\n",
    "    \n",
    "\n",
    "df = load_data('../data/sdg/retail-data/by-day/2011-*.csv')\n",
    "print(f\"df.count = {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2ff27a7-179e-460a-ad4d-6b24f015db0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fa988b-f672-4935-8d66-1dd8cbf99ece",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Using Mlib \n",
    "\n",
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f887b51-4422-4660-80fc-650fb8c69b0b",
   "metadata": {},
   "source": [
    "Add a new column: \"day of week\" and split to train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e8f7ec9-c948-43d0-99dc-46b693e60f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:===============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:test ratio: 2.341415438962707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_format, col\n",
    "preppedDataFrame = df\\\n",
    "  .na.fill(0)\\\n",
    "  .withColumn(\"day_of_week\", date_format(col(\"InvoiceDate\"), \"EEEE\"))\n",
    "  #.coalesce(5)\n",
    "\n",
    "# split to train and test:\n",
    "trainDataFrame,testDataFrame  = preppedDataFrame.randomSplit([0.7, 0.3])\n",
    "\n",
    "# we could also split using other criteria:\n",
    "# trainDataFrame = preppedDataFrame\\\n",
    "#   .where(\"InvoiceDate < '2011-07-01'\")\n",
    "# testDataFrame = preppedDataFrame\\\n",
    "#   .where(\"InvoiceDate >= '2011-07-01'\")\n",
    "\n",
    "print(f\"train:test ratio: {trainDataFrame.count()/testDataFrame.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888e3741-a059-49dc-a3af-80be1788457c",
   "metadata": {},
   "source": [
    "convert day of week \"Mon\" -> 2 -> one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19ab6915-328a-47db-b3d5-f2dfa73f5b4d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "day_indexer = StringIndexer()\\\n",
    "  .setInputCol(\"day_of_week\")\\\n",
    "  .setOutputCol(\"day_of_week_index\")\n",
    "\n",
    "country_indexer = StringIndexer()\\\n",
    "  .setInputCol(\"Country\")\\\n",
    "  .setOutputCol(\"country_index\")\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "encoder = OneHotEncoder()\\\n",
    "  .setInputCol(\"day_of_week_index\")\\\n",
    "  .setOutputCol(\"day_of_week_encoded\")\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "#  add \"features\" column that contains the input columns as elements in a vector.\n",
    "# Not very exciting, right?\n",
    "vectorAssembler = VectorAssembler()\\\n",
    "  .setInputCols([\"UnitPrice\", \"Quantity\", \"day_of_week_encoded\"])\\\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "# Read about pipelines here: https://spark.apache.org/docs/latest/ml-pipeline.html\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "transformationPipeline = Pipeline()\\\n",
    "  .setStages([day_indexer, country_indexer, encoder, vectorAssembler])\n",
    "\n",
    "fittedPipeline = transformationPipeline.fit(trainDataFrame)\n",
    "transformedTraining = fittedPipeline.transform(trainDataFrame)\n",
    "tranformedTest = fittedPipeline.transform(testDataFrame)\n",
    "\n",
    "# Let's drop unused columns. \n",
    "# This reduces the amount of needed memory so improving performance.\n",
    "transformedTraining = transformedTraining.drop('day_of_week').drop('day_of_week_encoded').drop('day_of_week_index'). drop('CustomerID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e02ac748-737d-4360-8c4e-db3c285679e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, Country: string, country_index: double, features: vector]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Caching the transfored DF will save a lot of time when reusing it (e.g. for hyper param tuning)\n",
    "transformedTraining.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d53770-3a86-4532-b450-c6f1b5fe7e5b",
   "metadata": {},
   "source": [
    "In Spark, training machine learning models is a two phase process. First we initialize an untrained model, then we\n",
    "train it. There are always two types for every algorithm in MLlibâ€™s DataFrame API. The algorithm Kmeans and then the\n",
    "trained version which is a KMeansModel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6f1c5f-16cc-40eb-a9e8-ca701c88b7a2",
   "metadata": {},
   "source": [
    "## K means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efd35788-5d6a-4a5d-ace7-988c8f2e68e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "kmeans = KMeans()\\\n",
    "  .setK(6)\\\n",
    "  .setSeed(1)\n",
    "\n",
    "kmModel = kmeans.fit(transformedTraining)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "638788da-ef32-4d3e-9d45-d93e52224dbf",
   "metadata": {},
   "source": [
    "TODO:continue here.\n",
    "consult the book, and only then look at the student's submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48da489d-56dd-4354-92b3-c520eb26dc28",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Supervised learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a084bc-e420-45c0-9e7d-0bc57ee19072",
   "metadata": {},
   "source": [
    "## Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04038c37-c07e-43a4-8489-fec9e0fe45ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['InvoiceNo',\n",
       " 'StockCode',\n",
       " 'Description',\n",
       " 'Quantity',\n",
       " 'InvoiceDate',\n",
       " 'UnitPrice',\n",
       " 'Country',\n",
       " 'country_index',\n",
       " 'features']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformedTraining.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3029b7ab-d114-451c-b476-84b74dadd393",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(labelCol=\"country_index\",featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea3dec15-baa7-4d7b-9cc9-dba8408d351d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can see a list of all hyperparams of LogisticRegression\n",
    "# print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e72d953-690e-4946-a84b-10dc02c8c983",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/23 15:06:18 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/02/23 15:06:18 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fittedLR = lr.fit(transformedTraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec5ba721-71f6-4c3b-8cce-a6c0799c55f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 167:=============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|country_index|     avg(prediction)|\n",
      "+-------------+--------------------+\n",
      "|          8.0|                 0.0|\n",
      "|          0.0|8.581676287434812E-4|\n",
      "|          7.0|                 0.0|\n",
      "|         18.0|                 0.0|\n",
      "|          1.0|                 0.0|\n",
      "|         34.0|                 0.0|\n",
      "|          4.0|                 0.0|\n",
      "|         23.0|                 0.0|\n",
      "|         31.0|                 0.0|\n",
      "|         11.0|                 0.0|\n",
      "|         21.0|                 0.0|\n",
      "|         14.0|                 0.0|\n",
      "|         22.0|0.056179775280898875|\n",
      "|         19.0|                 0.0|\n",
      "|          3.0|                 0.0|\n",
      "|         28.0|                 0.0|\n",
      "|          2.0|                 0.0|\n",
      "|         17.0|                 0.0|\n",
      "|         27.0|                 0.0|\n",
      "|         10.0|                 0.0|\n",
      "|         13.0|                 0.0|\n",
      "|          6.0|                 0.0|\n",
      "|         20.0|                 0.0|\n",
      "|          5.0|0.020718232044198894|\n",
      "|         15.0|                 0.0|\n",
      "|         24.0|                 0.0|\n",
      "|          9.0|                 0.0|\n",
      "|         26.0|                 0.0|\n",
      "|         16.0|                 0.0|\n",
      "|         12.0|                 0.0|\n",
      "|         35.0|                 0.0|\n",
      "|         25.0|                 0.0|\n",
      "|         30.0|                 0.0|\n",
      "|         29.0|                 0.0|\n",
      "|         33.0|                 0.0|\n",
      "|         32.0|                 0.0|\n",
      "|         36.0|                 0.0|\n",
      "+-------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fittedLR.transform(tranformedTest).select(\"country_index\", \"prediction\").\\\n",
    "groupBy(\"country_index\").avg(\"prediction\").show(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f895beb3-66c9-43cc-b4dc-3c8226cb17fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_index</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   country_index  prediction\n",
       "0            5.0         5.0\n",
       "1            5.0         5.0\n",
       "2            5.0         5.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fittedLR.transform(tranformedTest).select(\"country_index\", \"prediction\")\\\n",
    ".filter((col('country_index') == col('prediction')) & (col('prediction') != 0)).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eba0864-361d-4f18-bcd8-66b8d359c1a7",
   "metadata": {},
   "source": [
    "## Tuning the hyper-params\n",
    "\n",
    "TODO: reminder why this tuning is needed, and when is it enough\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852ebb48-5325-47bb-9019-644e6bfde75d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
