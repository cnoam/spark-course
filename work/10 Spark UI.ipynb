{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Apache Spark UI Tutorial\n",
    "## Introduction\n",
    "\n",
    "This tutorial will help you understand Spark's Web UI - your dashboard for monitoring and debugging Spark applications. The Spark UI provides valuable insights into your application's performance, resource usage, and execution flow.\n",
    "\n",
    "All information on this notebook is based on Apache Spark 3.5 UI Guide, available at:\n",
    "\n",
    "https://spark.apache.org/docs/latest/web-ui.html\n",
    "\n",
    "## Getting Started with Spark UI\n",
    "\n",
    "The Spark UI automatically launches when you start a Spark application and is typically available at http://localhost:4040."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T19:09:37.204889200Z",
     "start_time": "2025-05-13T19:09:37.103116600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bugo5\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pyspark\\context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark UI is available at: http://localhost:4040\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import random\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkUIJobsTab\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark UI is available at: http://localhost:4040\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. The Jobs Tab: Understanding Your Spark Workload\n",
    "\n",
    "The Jobs tab is your primary view into Spark's execution. A \"job\" is created whenever you execute an action on your data (like `collect()`, `count()`, or `save()`).\n",
    "\n",
    "When you visit the Jobs tab, you'll see:\n",
    "\n",
    "- A list of all jobs with their status (running, completed, or failed)\n",
    "- Execution times showing how long each job took\n",
    "- Breakdown of stages within each job\n",
    "- Links to more detailed views\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Jobs Tab Description](res/jobs_tab.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Key Components of the Jobs Tab:\n",
    "\n",
    "- **Job ID**: Each job gets a unique numerical identifier, starting at 0 for the first job in your application. In our example, we see jobs with IDs 4, 5, 6, and 7.\n",
    "- **Description**: This column shows which operation triggered the job. Notice how operations like count() and show() appear here with line numbers from your code. This helps you connect specific actions in your code to the jobs they create.\n",
    "- **Duration**: How long each job took to execute, measured in seconds. This is invaluable for identifying performance bottlenecks. In our example, durations range from 0.4 seconds to 17 seconds.\n",
    "- **Stages**: Spark breaks each job into \"stages\" that can be executed independently. The \"Succeeded/Total\" format shows how many stages have completed out of the total. Notice how some jobs have just 1 stage while others have 3 stages - more complex operations typically require more stages.\n",
    "- **Tasks**: These are the individual units of work distributed across your cluster. Simple jobs might have few tasks, while complex operations on large datasets can have thousands. The \"Succeeded/Total\" format shows completion progress.\n",
    "\n",
    "### Active vs. Completed Jobs:\n",
    "The Jobs tab separates currently running jobs from completed ones:\n",
    "\n",
    "- **Active Jobs**: Job #7 is still running, as indicated by the \"running\" status in the Tasks column and incomplete stages (0/2).\n",
    "- **Completed Jobs**: Jobs #4, 5, and 6 have finished execution. You can see all their tasks have succeeded (e.g., 9/9 and 1/1) and their stages are complete.\n",
    "\n",
    "### What This Tells Us About Spark Execution:\n",
    "\n",
    "Simple actions like show with small results (Job #6) are quick and require minimal processing (just 0.4s and one stage).\n",
    "More complex operations (Jobs #4 and #5) require multiple stages and more tasks, which often means data shuffling between stages.\n",
    "In-progress jobs can be monitored in real-time to track their execution progress.\n",
    "\n",
    "When developing Spark applications, pay close attention to jobs with long durations or many stages, as these are prime candidates for optimization.\n",
    "In the next sections, we'll look at what happens inside these jobs by examining the Stages tab and visualizations of execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Let's create a code example to demonstrate :\n",
    "run this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting calculation...\n",
      "Category counts:\n",
      "+----------+-----+\n",
      "|  category|count|\n",
      "+----------+-----+\n",
      "|category_2| 2000|\n",
      "|category_4| 2000|\n",
      "|category_1| 2000|\n",
      "|category_0| 2000|\n",
      "|category_3| 2000|\n",
      "+----------+-----+\n",
      "\n",
      "Category statistics:\n",
      "+----------+----------+\n",
      "|  category|sum(value)|\n",
      "+----------+----------+\n",
      "|category_2|    100462|\n",
      "|category_4|     99193|\n",
      "|category_1|     99836|\n",
      "|category_0|    101568|\n",
      "|category_3|    100736|\n",
      "+----------+----------+\n",
      "\n",
      "Top values above 50:\n",
      "+----+----------+-----+\n",
      "|  id|  category|value|\n",
      "+----+----------+-----+\n",
      "|2081|category_1|  100|\n",
      "|6705|category_0|  100|\n",
      "|2443|category_3|  100|\n",
      "| 668|category_3|  100|\n",
      "| 722|category_2|  100|\n",
      "|7125|category_0|  100|\n",
      "|7796|category_1|  100|\n",
      "|8553|category_3|  100|\n",
      "|5915|category_0|  100|\n",
      "| 234|category_4|  100|\n",
      "+----+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "1. Go to http://localhost:4040\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import random\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Task Analysis Example\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a larger DataFrame to see multiple tasks\n",
    "data = [(i, f\"category_{i % 5}\", random.randint(1, 100)) \n",
    "        for i in range(10000)]  # 10,000 rows will create multiple tasks\n",
    "df = spark.createDataFrame(data, [\"id\", \"category\", \"value\"])\n",
    "\n",
    "# Repartition to control the number of tasks\n",
    "df = df.repartition(8)  # Create 8 partitions for better task visualization\n",
    "\n",
    "# Perform a computation that will create visible tasks\n",
    "print(\"Starting calculation...\")\n",
    "\n",
    "# Action 1: Count per category (will show clear task divisions)\n",
    "result1 = df.groupBy(\"category\").count()\n",
    "print(\"Category counts:\")\n",
    "result1.show()\n",
    "\n",
    "# Action 2: Calculate statistics (creates more complex tasks)\n",
    "result2 = df.groupBy(\"category\").agg(\n",
    "    {\"value\": \"min\", \"value\": \"max\", \"value\": \"avg\", \"value\": \"sum\"}\n",
    ")\n",
    "print(\"Category statistics:\")\n",
    "result2.show()\n",
    "\n",
    "# Action 3: Filter and sort (different task pattern)\n",
    "result3 = df.filter(df.value > 50).sort(\"value\", ascending=False)\n",
    "print(\"Top values above 50:\")\n",
    "result3.show(10)\n",
    "\n",
    "print(\"1. Go to http://localhost:4040\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the Jobs tab in the Spark UI where you will see multiple jobs completed. You can go into detail of every job! \n",
    "\n",
    "Choose job 15 (Notice the interesting DAG. We will explain later in the tutorial how to read them. Go back and try to analyze it after learning the DAG section!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Jobs Tab Description](res/job_tavnew.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the job consisted of two stages (one completed, one skipped). The completed stage ran 8 parallel tasks.\n",
    "\n",
    "1. **Parallel Processing:**\n",
    "   - Our code used `.repartition(8)` to explicitly create 8 partitions\n",
    "   - The completed stage shows 8/8 tasks, confirming Spark honored our partitioning\n",
    "   - Each task processed approximately 1/8th of our 10,000 rows\n",
    "\n",
    "2. **Different Job Patterns:**\n",
    "   - **Action 1 (groupBy-count):** Created a shuffle to group by category, requiring data with the same category to be brought together\n",
    "   - **Action 2 (multiple aggregations):** Generated more complex processing with min, max, avg, and sum calculations\n",
    "   - **Action 3 (filter-sort):** Created a different pattern with the sort operation requiring data exchange\n",
    "\n",
    "3. **Data Movement:**\n",
    "   - The shuffle read/write metrics show how our data needed to be reorganized, especially for:\n",
    "     - Grouping by category (in result1 and result2)\n",
    "     - Sorting values (in result3)\n",
    "\n",
    "4. **Optimization:**\n",
    "   - Some stages were skipped because Spark could reuse information from earlier stages\n",
    "   - Our explicit partitioning into 8 parts helped distribute the workload evenly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. Understanding Execution Through Visualizations\n",
    "\n",
    "### Event Timeline\n",
    "\n",
    "The Event Timeline provides a chronological view of your Spark application's execution, showing:\n",
    "\n",
    "- When each job, stage, and task ran\n",
    "- How long each component took to execute\n",
    "- Parallel execution across your cluster\n",
    "- Wait times and potential bottlenecks\n",
    "\n",
    "Pay attention to \"gaps\" in the timeline, which may indicate scheduling delays or resource contention. If you see tasks executing serially rather than in parallel, you might need to **adjust your partitioning**. Long-running tasks that delay an entire stage completion are often signs of data skew.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Jobs Tab Description](res/Event_timeLine.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1. **Time Progression**: The horizontal axis shows time, with timestamps marking specific points (17:42 through 17:50 in this example).\n",
    "\n",
    "2. **Component Layers**: The timeline is divided into horizontal sections:\n",
    "   - **Executors**: Shows when executors are added or removed from your application\n",
    "   - **Jobs**: Shows when jobs are running, succeeded, or failed\n",
    "\n",
    "3. **Color Coding**: Different colors represent different states:\n",
    "   - **Blue** (for Executors): Added executors\n",
    "   - **Pink/Red**: Removed executors or failed jobs\n",
    "   - **Blue** (for Jobs): Succeeded jobs\n",
    "   - **Green**: Currently running jobs\n",
    "\n",
    "4. **Key Events Visible**:\n",
    "   - At the beginning, you can see when executors were added to the application\n",
    "   - Throughout the timeline, small blue bars represent completed jobs\n",
    "   - On the right side, a green \"count\" job is currently running\n",
    "\n",
    "### What to Look For in the Event Timeline:\n",
    "\n",
    "The Event Timeline helps you identify:\n",
    "\n",
    "- **Executor Lifecycle**: When executors join or leave your application\n",
    "- **Job Execution Patterns**: When jobs start and finish\n",
    "- **Idle Periods**: Gaps in the timeline where no activity occurs\n",
    "- **Concurrency**: Multiple jobs or tasks running in parallel\n",
    "- **Long-Running Operations**: Jobs that span significant portions of the timeline\n",
    "\n",
    "While this example shows a relatively simple execution pattern, in more complex applications you would see many more parallel activities, helping you visualize how effectively your application utilizes cluster resources.\n",
    "\n",
    "The Event Timeline is particularly valuable when diagnosing performance issues, as it can reveal execution bottlenecks, resource contention, or inefficient scheduling patterns.\n",
    "\n",
    "You can enable zooming (checkbox at the top) to examine specific time periods in more detail, which is especially useful for complex applications with many overlapping activities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "## DAG Visualization: Understanding Spark's Execution Plan\n",
    "\n",
    "The Directed Acyclic Graph (DAG) visualization provides a clear picture of how Spark structures the operations in your job. This visualization is extremely valuable for understanding the logical flow of data through your Spark application\n",
    "\n",
    "- Each node represents an operation (like map, filter, join)\n",
    "- Arrows show data flow and dependencies\n",
    "- Stages are separated by shuffle boundaries (where data needs to be redistributed)\n",
    "\n",
    "The DAG helps you identify expensive operations like shuffle-heavy joins or cartesian products. Multiple arrows converging on a single operation often indicate a join or aggregation that could become a bottleneck. Wide transformations (those that require shuffles) create stage boundaries and are typically more expensive than narrow transformations..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Jobs Tab Description](res/DAG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### What You're Seeing in This DAG:\n",
    "\n",
    "This visualization shows how a Spark job is broken down into multiple stages (Stage 10 and Stage 11) and the operations within each stage:\n",
    "\n",
    "1. **Stage Boundaries**: Each pink box represents a distinct stage. Stages are separated at points where data must be redistributed across the cluster (called shuffle boundaries).\n",
    "\n",
    "2. **Operations Within Stages**: The blue boxes represent specific operations:\n",
    "   - **Parallelize**: Creating a distributed dataset from data\n",
    "   - **Scan**: Reading data from a source\n",
    "   - **Exchange**: Redistributing data across the cluster (shuffle operation)\n",
    "   - **WholeStageCodegen**: Spark's optimization that compiles multiple operations into efficient bytecode\n",
    "   - **MapPartitionsInternal**: An internal transformation operating on each partition\n",
    "\n",
    "3. **Data Flow Direction**: The arrows show the direction of data flow, always moving downward within stages and then across to the next stage.\n",
    "\n",
    "4. **Shuffle Boundary**: The curved line connecting the \"Exchange\" operation from Stage 10 to Stage 11 represents a shuffle - a point where data must be redistributed across the cluster. Shuffles create stage boundaries in Spark.\n",
    "\n",
    "### Why This Matters:\n",
    "\n",
    "The DAG visualization reveals important insights about your Spark application:\n",
    "\n",
    "- **Performance Bottlenecks**: Exchange operations (shuffles) are often performance bottlenecks because they involve network transfer and disk I/O. In this example, there's a shuffle between Stages 10 and 11.\n",
    "\n",
    "- **Optimization Opportunities**: The presence of \"WholeStageCodegen\" indicates that Spark is applying code generation optimizations to improve performance by combining multiple operations.\n",
    "\n",
    "- **Execution Dependencies**: You can see which operations must complete before others can begin. For example, in Stage 10, the \"Scan\" must complete before \"WholeStageCodegen (1)\" can start.\n",
    "\n",
    "- **Parallelization Potential**: Operations within a stage can be parallelized, but stages must execute in sequence when there are dependencies between them.\n",
    "\n",
    "Understanding the DAG helps you reason about how your code translates into actual execution steps. When optimizing Spark applications, you'll often refer to this visualization to identify opportunities for improvement, such as reducing the number of shuffle operations or ensuring that data is properly partitioned.\n",
    "\n",
    "In complex jobs with many stages, the DAG becomes even more valuable as it helps you visualize the execution flow that would otherwise be difficult to comprehend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Let's create a code example to demonstrate:\n",
    "run this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after transformation:\n",
      "+---------+---------+\n",
      "|  product|avg_value|\n",
      "+---------+---------+\n",
      "|product_1|    265.0|\n",
      "|product_2|    260.0|\n",
      "|product_0|    255.0|\n",
      "+---------+---------+\n",
      "\n",
      "\n",
      "Check the Spark UI at http://localhost:4040\n",
      "Go to the Jobs tab, click on the job, then view the DAG Visualization\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Simple DAG Example\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = [(i, f\"product_{i % 3}\", i * 10) for i in range(50)]\n",
    "df = spark.createDataFrame(data, [\"id\", \"product\", \"value\"])\n",
    "\n",
    "# Apply a series of transformations that will create a visible DAG\n",
    "# Step 1: Filter the data\n",
    "filtered_df = df.filter(col(\"value\") > 20)\n",
    "\n",
    "# Step 2: Group by product\n",
    "summary_df = filtered_df.groupBy(\"product\").agg(\n",
    "    avg(\"value\").alias(\"avg_value\")\n",
    ")\n",
    "\n",
    "# Execute an action to materialize the DAG\n",
    "print(\"Results after transformation:\")\n",
    "summary_df.show()\n",
    "\n",
    "print(\"\\nCheck the Spark UI at http://localhost:4040\")\n",
    "print(\"Go to the Jobs tab, click on the job, then view the DAG Visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to Job ID 3 in the describtion coloum and click on the DAG Visualization link to see the DAG.\n",
    "you should see a DAG like this:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Jobs Tab Description](res/dag_vis_new.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The parallelize operation at the top represents your initial DataFrame creation:\n",
    "\n",
    "        data = [(i, f\"product_{i % 3}\", i * 10) for i in range(50)]\n",
    "\n",
    "        df = spark.createDataFrame(data, [\"id\", \"product\", \"value\"])\n",
    "\n",
    "2. The mapPartitions operations correspond to your filter transformation:\\\n",
    "\n",
    "        filtered_df = df.filter(col(\"value\") > 20)\n",
    "\n",
    "3. The WholeStageCodegen boxes show Spark optimizie your code by compiling operations together for better performance.\n",
    "\n",
    "4. The InMemoryTableScan operations represent Spark reading and processing your DataFrame data.\n",
    "\n",
    "5. The BroadcastExchange at the bottom is preparing data for the groupBy operation, which requires shuffling data so all products with the same name are processed together:\n",
    "\n",
    "        summary_df = filtered_df.groupBy(\"product\").agg(\n",
    "            avg(\"value\").alias(\"avg_value\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. The Stages Tab: Diving Deeper Into Execution\n",
    "\n",
    "Stages are sets of tasks that can be executed in parallel without data shuffling. The Stages tab shows:\n",
    "\n",
    "- Detailed metrics for each stage\n",
    "- Input/output data sizes\n",
    "- Task distribution and execution times\n",
    "- Whether stages completed successfully or failed\n",
    "\n",
    "When examining the Stages tab, look for:\n",
    "\n",
    "- **Task skew**: When some tasks take much longer than others in the same stage\n",
    "- **Spill metrics**: Data spilled to disk indicates memory pressure\n",
    "- **Shuffle read/write sizes**: Large shuffles can slow down your application\n",
    "- **Input/output records**: Help identify data flow bottlenecks\n",
    "\n",
    "Clicking on a specific stage provides a task-level view where you can see outliers and understand the distribution of work across your cluster.\n",
    "This view is invaluable for understanding the actual work happening in your Spark application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Jobs Tab Description](res/Stage_image1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Understanding the Stages Tab Structure:\n",
    "\n",
    "The Stages tab is divided into three key sections, each showing stages in different states:\n",
    "\n",
    "1. **Active Stages**: Currently running stages\n",
    "   - In our example, Stage 13 is active with 0/4 tasks completed (4 running)\n",
    "   - Shows real-time progress of execution\n",
    "\n",
    "2. **Pending Stages**: Stages waiting for resources or dependent stages to complete\n",
    "   - Stage 14 is pending with 0/200 tasks completed\n",
    "   - These stages are queued but haven't started execution yet\n",
    "\n",
    "3. **Completed Stages**: Stages that have finished execution\n",
    "   - Our example shows multiple completed stages (5-12)\n",
    "   - Provides historical performance data for analysis\n",
    "\n",
    "### Key Metrics To Monitor:\n",
    "\n",
    "Each stage displays several important metrics:\n",
    "\n",
    "- **Stage ID**: Unique identifier for each stage\n",
    "- **Pool Name**: The scheduler pool handling this stage (affects resource allocation)\n",
    "- **Description**: The operation being performed (like \"show\" or \"foreach\")\n",
    "- **Duration**: How long the stage took to execute (ranging from 0.2s to 64ms in our example)\n",
    "- **Tasks**: Shows completed/total tasks and visualizes progress\n",
    "- **Shuffle Read/Write**: Amount of data transferred during shuffle operations\n",
    "  - Note stages with significant shuffle write (1205.5 KiB) which indicate data redistribution\n",
    "\n",
    "### What This Tells Us About Performance:\n",
    "\n",
    "Looking at the completed stages, we can observe:\n",
    "- Most stages complete quickly (0.3s-0.5s)\n",
    "- Stage 6 took significantly longer (64ms) than others - a potential bottleneck\n",
    "- Several stages have identical shuffle write sizes (1205.5 KiB), suggesting similar data volume processing\n",
    "- Task counts vary (some have 1 task, others have 4), indicating different levels of parallelism\n",
    "\n",
    "By examining these metrics, you can identify which stages contribute most to your job's overall execution time and focus your optimization efforts accordingly.\n",
    "\n",
    "You can click on any stage ID to view more detailed information about that specific stage, including a visual representation of its internal operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Stage Detail View: Understanding Operation Flow\n",
    "\n",
    "When you click on a specific stage in the Stages tab, you'll see a detailed view of the operations within that stage, as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Jobs Tab Description](res/stage_image2_dag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Decoding the Stage Internals:\n",
    "\n",
    "This visualization shows the execution plan for Stage 15, revealing how data flows through various operations:\n",
    "\n",
    "1. **Parallel Input Paths**: The stage begins with two parallel data paths, each starting with a \"ShuffledRowRDD\" operation followed by \"Exchange\" - indicating data coming from previous stages through a shuffle.\n",
    "\n",
    "2. **Data Transformations**: Each path goes through a series of operations:\n",
    "   - **MapPartitionsRDD**: Applying transformations to each partition\n",
    "   - **WholeStageCodegen**: Spark's optimization that combines operations into efficient bytecode\n",
    "   - **Exchange**: Points where data is redistributed across the cluster\n",
    "\n",
    "3. **Data Flow Convergence**: The two paths merge at the \"ZippedPartitionsRDD2\" operation, combining data from both sources.\n",
    "\n",
    "4. **Final Processing**: After merging, the data goes through additional mapping operations and a final exchange before completing the stage.\n",
    "\n",
    "### Performance Insights From This View:\n",
    "\n",
    "This detailed view reveals important aspects of Spark's execution strategy:\n",
    "\n",
    "- **Code Generation Optimization**: Multiple \"WholeStageCodegen\" boxes show where Spark compiles operations together for efficiency - operations within these boundaries execute much faster.\n",
    "\n",
    "- **Data Shuffling Points**: Each \"Exchange\" operation represents a potential bottleneck where data moves between executors.\n",
    "\n",
    "- **Operation Dependencies**: The arrows show which operations must wait for others to complete, revealing the critical path through your execution.\n",
    "\n",
    "- **Operation Numbering**: Each operation has a unique identifier (like \"[35]\", \"[41]\", etc.) that helps trace specific transformations back to your code.\n",
    "\n",
    "This visualization is invaluable for understanding complex transformations and identifying optimization opportunities. When tuning Spark applications, you'll often examine these diagrams to spot inefficient patterns like excessive shuffling or missed opportunities for pipelining operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4. The Storage Tab: Managing Cached Data\n",
    "\n",
    "The Storage tab provides insight into how Spark manages cached data, which is crucial for optimizing performance by avoiding redundant computations.\n",
    "\n",
    "In the Storage tab, you can see:\n",
    "\n",
    "- Which datasets are cached\n",
    "- How much memory they're using\n",
    "- Whether they're stored in memory, on disk, or both\n",
    "- The fraction of the dataset that's cached\n",
    "\n",
    "If the \"Fraction Cached\" is less than 100%, it means some partitions couldn't fit in memory and were either spilled to disk or not cached at all, depending on your storage level. Different storage levels (like MEMORY_ONLY, MEMORY_AND_DISK, or DISK_ONLY) affect both performance and resilience.\n",
    "\n",
    "The \"Size in Memory\" vs \"Size on Disk\" comparison helps you understand serialization overhead and compression efficiency. Partitions that are well-distributed will show more even storage across executors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Jobs Tab Description](res/storage_image1.png)\n",
    "![Jobs Tab Description](res/Storage_tab.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Understanding the Storage Tab:\n",
    "\n",
    "The Storage tab displays all cached RDDs, DataFrames, and Datasets in your application, along with key metrics that help you monitor your memory usage:\n",
    "\n",
    "1. **RDD/Table Information**:\n",
    "   - **ID**: Unique identifier for each cached object\n",
    "   - **Name**: Descriptive name of the dataset (e.g., \"rdd\" or \"LocalTableScan [count#7, name#8]\")\n",
    "\n",
    "2. **Storage Characteristics**:\n",
    "   - **Storage Level**: How data is stored - shown in our example:\n",
    "     - \"Memory Serialized 1x Replicated\": Stored in memory in serialized format\n",
    "     - \"Disk Serialized 1x Replicated\": Stored on disk in serialized format\n",
    "\n",
    "3. **Partition Information**:\n",
    "   - **Cached Partitions**: Number of partitions stored (5 and 3 respectively)\n",
    "   - **Fraction Cached**: Percentage of data actually cached (both at 100%)\n",
    "\n",
    "4. **Size Metrics**:\n",
    "   - **Size in Memory**: Space used in RAM (236.0 B for the first RDD)\n",
    "   - **Size on Disk**: Space used on disk (2.1 KiB for the second RDD)\n",
    "\n",
    "### Why the Storage Tab Is Important:\n",
    "\n",
    "This tab helps you:\n",
    "\n",
    "- **Verify Caching Strategy**: Confirm that your `.cache()` or `.persist()` operations worked as expected\n",
    "- **Monitor Memory Usage**: Ensure you're not caching too much data and risking out-of-memory errors\n",
    "- **Diagnose Performance Issues**: If a query is slow despite caching, check if data was actually cached\n",
    "- **Optimize Storage Levels**: Make informed decisions about which storage level to use (memory-only, disk-only, or combined)\n",
    "\n",
    "For larger datasets, you'll also see when data partially spills to disk or when some partitions couldn't be cached due to memory constraints.\n",
    "\n",
    "You can click on any RDD name to see a more detailed view showing exactly how the data is distributed across executors and individual partition sizes.\n",
    "\n",
    "For large-scale applications, proper caching can dramatically improve performance by reusing data across operations rather than recomputing it each time. The Storage tab gives you visibility into this crucial optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 5. The SQL Tab: Analyzing Query Performance\n",
    "\n",
    "The SQL tab in Spark UI provides visibility into all Spark SQL operations in your application, including DataFrame operations which are translated into SQL behind the scenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Jobs Tab Description](res/sql_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Understanding the SQL Tab:\n",
    "\n",
    "The SQL tab shows a list of all queries executed in your Spark application, with crucial information about each:\n",
    "\n",
    "1. **Query Identification**:\n",
    "   - **ID**: Each query receives a unique identifier (0, 1, 2 in our example)\n",
    "   - **Description**: Shows the operation executed (e.g., \"count at \\<console\\>:26\", \"createGlobalTempView at \\<console\\>:26\")\n",
    "\n",
    "2. **Execution Timeline**:\n",
    "   - **Submitted**: When the query was sent for execution\n",
    "   - **Duration**: How long each query took to run (ranging from 0.3s to 2s in our example)\n",
    "\n",
    "3. **Related Jobs**:\n",
    "   - **Job IDs**: Links to the Spark jobs created to execute this query\n",
    "   - Note how query ID 2 generated multiple jobs ([1][2][3][4][5]), indicating a more complex execution\n",
    "\n",
    "4. **Details Link**:\n",
    "   - The \"+details\" link allows you to dive deeper into each query\n",
    "   - Clicking it reveals the logical and physical plans for the query\n",
    "\n",
    "### What This Tells Us About Query Execution:\n",
    "\n",
    "Looking at this overview, we can observe:\n",
    "\n",
    "- The \"count\" operation (ID 0) took 2 seconds and generated one job\n",
    "- The \"createGlobalTempView\" operation (ID 1) was relatively quick at 0.3 seconds\n",
    "- The \"show\" operation (ID 2) was more complex, generating 5 different jobs and taking 2 seconds\n",
    "\n",
    "### Why the SQL Tab Is Valuable:\n",
    "\n",
    "The SQL tab helps you:\n",
    "\n",
    "- **Track SQL Performance**: Identify which queries are taking the longest time\n",
    "- **Connect Operations**: Link high-level DataFrame operations to the underlying Spark jobs\n",
    "- **Troubleshoot Issues**: When a query performs poorly, you can examine its execution details\n",
    "- **Optimize Queries**: By understanding the execution plan, you can modify your code for better performance\n",
    "\n",
    "When you click the \"+details\" link for a query, you'll see the logical and physical plans, which reveal how Spark interprets and optimizes your query. This deeper view shows operations like joins, filters, and projections, along with important optimization decisions made by the Spark SQL engine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 6. The Environment Tab: Configuration Details\n",
    "\n",
    "The Environment tab provides a comprehensive view of your Spark application's configuration settings, helping you understand exactly how Spark is set up to run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Jobs Tab Description](res/Environment_tab.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Key Components of the Environment Tab:\n",
    "\n",
    "The Environment tab is divided into several sections, with the most important being Runtime Information and Spark Properties:\n",
    "\n",
    "1. **Runtime Information**:\n",
    "   - **Java/Scala Versions**: In our example, we see Java 1.8.0_221 and Scala 2.12.8\n",
    "   - **Java Home**: Shows where the JVM is installed\n",
    "   - This information helps troubleshoot compatibility issues\n",
    "\n",
    "2. **Spark Properties**:\n",
    "   - **Application Configuration**: Settings like `spark.app.id` and `spark.app.name`\n",
    "   - **Execution Settings**: Properties that control how Spark runs your jobs\n",
    "   - **Critical Performance Parameters**:\n",
    "     - `spark.scheduler.mode`: \"FIFO\" in our example\n",
    "     - `spark.sql.catalogImplementation`: \"in-memory\"\n",
    "     - `spark.master`: \"local[*]\" indicating local mode execution\n",
    "\n",
    "### Important Settings to Monitor:\n",
    "\n",
    "Several key properties in this tab directly impact application performance:\n",
    "\n",
    "- **Resource Allocation**: While not explicitly configured in our example, you'd typically see memory settings like `spark.executor.memory` here\n",
    "- **Execution Control**: The `spark.scheduler.mode` determines how concurrent jobs are handled\n",
    "- **SQL Optimization**: `spark.sql.catalogImplementation` shows how Spark manages SQL metadata\n",
    "\n",
    "### Why This Information Matters:\n",
    "\n",
    "The Environment tab serves several crucial purposes:\n",
    "\n",
    "- **Troubleshooting**: When something isn't working, this is often the first place to check\n",
    "- **Verification**: Confirm your configuration settings were properly applied\n",
    "- **Documentation**: Provides a complete record of your application's environment\n",
    "- **Optimization**: Identify opportunities to tune settings for better performance\n",
    "\n",
    "For developers and administrators, the Environment tab is invaluable for understanding the exact conditions under which your Spark application is running. Many performance issues can be traced back to suboptimal configuration settings visible in this tab.\n",
    "\n",
    "In production environments, you'll often compare configurations across different applications to standardize settings or identify differences that might explain performance variations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 7. The Executors Tab: Resource Utilization\n",
    "\n",
    "The Executors tab provides insight into the worker processes that execute your Spark jobs, showing how computational resources are allocated and utilized across your cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Jobs Tab Description](res/exector_tab.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Understanding the Executors Tab:\n",
    "\n",
    "The Executors tab is divided into two main sections:\n",
    "\n",
    "1. **Summary Statistics**:\n",
    "   - **Active/Total/Dead Executors**: In our example, there are 3 active executors and 0 dead ones\n",
    "   - **Resource Allocation**: The cluster has 2 cores active and 5.9 KiB / 1.1 GiB of storage memory in use\n",
    "   - **Task Metrics**: 5 completed tasks with a total task time of 4 seconds (including 0.2 seconds of garbage collection)\n",
    "\n",
    "2. **Individual Executor Details**:\n",
    "   - **Executor ID**: Each executor has a unique identifier (0, 1, and \"driver\")\n",
    "   - **Status**: All executors show as \"Active\" in our example\n",
    "   - **Resource Allocation**: Each executor has 1 core and 2 KiB / 366.3 MiB of storage memory\n",
    "   - **Task Distribution**: Executor 1 has completed 3 tasks, Executor 0 has completed 2 tasks\n",
    "   - **Performance Metrics**: Task time and GC time help identify processing bottlenecks\n",
    "   - **Logs**: Links to stdout/stderr logs and thread dumps for debugging\n",
    "\n",
    "### Key Metrics to Monitor:\n",
    "\n",
    "Several important indicators of application health are visible here:\n",
    "\n",
    "- **Task Distribution**: Ideally, tasks should be evenly distributed across executors. In our example, tasks are reasonably balanced between Executors 0 and 1.\n",
    "- **Memory Usage**: Storage memory shows how much data is cached on each executor\n",
    "- **GC Time**: Garbage collection time as a proportion of total task time (0.2s out of 4s here) helps identify memory pressure\n",
    "- **Shuffle Read/Write**: In our example, there's no shuffle activity (all 0.0 B), but in data-intensive applications, high shuffle volumes can indicate potential performance issues\n",
    "\n",
    "### Why the Executors Tab Is Important:\n",
    "\n",
    "This tab helps you:\n",
    "\n",
    "- **Monitor Resource Utilization**: Ensure executors have appropriate memory and CPU allocation\n",
    "- **Identify Skew**: Detect when certain executors are processing significantly more data or tasks than others\n",
    "- **Troubleshoot Failures**: Quickly access logs when executors fail or tasks encounter errors\n",
    "- **Track Performance**: Monitor GC time and task execution metrics to optimize resource allocation\n",
    "\n",
    "For production Spark applications, regularly checking the Executors tab helps ensure your cluster is properly sized and resources are efficiently utilized across all worker nodes.\n",
    "\n",
    "In larger deployments, you might see dozens or hundreds of executors, making this view essential for identifying outliers or problematic nodes that could impact overall application performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "## Putting It All Together: Analyzing a Complete Workflow\n",
    "\n",
    "Let's create a comprehensive example to demonstrate how  aspects of the Spark UI work together:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T19:09:51.234126700Z",
     "start_time": "2025-05-13T19:09:37.234132Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bugo5\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pyspark\\context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+-----------------+-----------------+--------------+\n",
      "|       name|age|        avg_grade|   avg_attendance|subjects_taken|\n",
      "+-----------+---+-----------------+-----------------+--------------+\n",
      "| Student_35| 15|             98.0|             86.0|             1|\n",
      "| Student_42| 14|             93.2|             96.2|             2|\n",
      "|Student_176| 17|91.31818181818181|86.04545454545455|             2|\n",
      "| Student_61| 14|90.66666666666667|91.33333333333333|             2|\n",
      "|Student_107| 18|             90.0|            96.75|             2|\n",
      "|Student_193| 18|88.85714285714286|90.14285714285714|             2|\n",
      "|Student_186| 17|             87.5|             92.5|             1|\n",
      "| Student_79| 16|87.35714285714286|             86.5|             3|\n",
      "| Student_89| 14|87.27272727272727|             91.0|             3|\n",
      "| Student_75| 15|87.23076923076923|             87.0|             3|\n",
      "| Student_66| 14|87.15384615384616|85.84615384615384|             2|\n",
      "|Student_183| 14|87.07692307692308|86.84615384615384|             2|\n",
      "|Student_133| 14|             87.0|92.28571428571429|             2|\n",
      "|  Student_7| 16|86.85714285714286|             87.0|             2|\n",
      "| Student_76| 16|             86.5|90.83333333333333|             2|\n",
      "| Student_83| 16|             86.0|87.33333333333333|             2|\n",
      "| Student_10| 14|             86.0|             85.2|             3|\n",
      "|Student_136| 17|85.85714285714286|             87.0|             2|\n",
      "| Student_97| 14|85.83333333333333|86.66666666666667|             2|\n",
      "|Student_150| 18|             85.6|85.53333333333333|             3|\n",
      "+-----------+---+-----------------+-----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "Check the Spark UI at http://localhost:4040\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Set environment variables to fix Python worker connection issues\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import random\n",
    "\n",
    "# Initialize a Spark session with fixed connection parameters\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkUIComprehensiveExample\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .config(\"spark.python.worker.timeout\", \"600\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create example datasets - using smaller sizes\n",
    "students = spark.createDataFrame([\n",
    "    (i, f\"Student_{i}\", random.randint(14, 18))\n",
    "    for i in range(200)  # Reduced from 1000\n",
    "], [\"id\", \"name\", \"age\"])\n",
    "\n",
    "grades = spark.createDataFrame([\n",
    "    (random.randint(1, 200),  # Reduced range to match student IDs\n",
    "     random.choice(['Math', 'Science', 'History']),\n",
    "     random.randint(60, 100))\n",
    "    for _ in range(1000)  # Reduced from 5000\n",
    "], [\"student_id\", \"subject\", \"grade\"])\n",
    "\n",
    "attendance = spark.createDataFrame([\n",
    "    (random.randint(1, 200),  # Reduced range to match student IDs\n",
    "     random.choice(['Math', 'Science', 'History']),\n",
    "     random.randint(70, 100))\n",
    "    for _ in range(1500)  # Reduced from 7000\n",
    "], [\"student_id\", \"subject\", \"attendance_pct\"])\n",
    "\n",
    "# Cache one of our tables\n",
    "grades.cache()\n",
    "grades.count()  # Materialize the cache\n",
    "\n",
    "# Make them available for SQL\n",
    "students.createOrReplaceTempView(\"students\")\n",
    "grades.createOrReplaceTempView(\"grades\")\n",
    "attendance.createOrReplaceTempView(\"attendance\")\n",
    "\n",
    "# Run a complex query\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        s.name,\n",
    "        s.age,\n",
    "        AVG(g.grade) as avg_grade,\n",
    "        AVG(a.attendance_pct) as avg_attendance,\n",
    "        COUNT(DISTINCT g.subject) as subjects_taken\n",
    "    FROM students s\n",
    "    JOIN grades g ON s.id = g.student_id\n",
    "    JOIN attendance a ON s.id = a.student_id AND g.subject = a.subject\n",
    "    GROUP BY s.name, s.age\n",
    "    HAVING AVG(g.grade) > 80 AND AVG(a.attendance_pct) > 85\n",
    "    ORDER BY avg_grade DESC, avg_attendance DESC\n",
    "\"\"\")\n",
    "\n",
    "# Show the results\n",
    "result.show()\n",
    "\n",
    "print(\"\\nCheck the Spark UI at http://localhost:4040\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## let's now exmaine the UI and understand our results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Jobs Tab Description](res/job_tab_finalnew.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **What we're seeing**:\n",
    " Each row represents a job that Spark created from our student grades analysis code.\n",
    "\n",
    "2. **Job sources**: The \"Description\" column shows what triggered each job - we can see the \"count\" operations from our `grades.count()` and the \"showString\" from our `result.show()`.\n",
    "\n",
    "3. **Performance at a glance**: The \"Duration\" column tells us how long each job took to run. Notice some jobs completed quickly (0.1s) while others took longer (3s).\n",
    "\n",
    "4. **Join operations**: Jobs 3 and 5 show \"broadcast exchange\" operations - these are from the SQL joins between our students, grades, and attendance tables.\n",
    "\n",
    "5. **Work distribution**: The \"Tasks\" column shows how Spark divided the work into parallel tasks, which helps process data faster.\n",
    "\n",
    "let's look at the Storage tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Jobs Tab Description](res/storage_new.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Cache confirmation**: The RDD from our grades.cache() call is visible here.\n",
    "- **Storage strategy**: \"Disk Memory Deserialized\" means our data is stored both in memory and on disk for faster access.\n",
    "- **Memory usage**: The data takes up just 6.2 KiB and is 100% cached, showing our caching worked perfectly.\n",
    "- **Partitioning**: The data is split into 2 partitions for parallel processing.\n",
    "\n",
    "Finally let us look at the Stages:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Jobs Tab Description](res/stages_1new.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- Stages 0 and 2 show the count() operations from our code where we ran grades.count() to materialize the cache.\n",
    "- The \"broadcast exchange\" stages (7 and 4) represent how Spark handled our three-way JOIN between students, grades, and attendance tables in our SQL query.\n",
    "- The \"showString\" stages correspond to our result.show() call at the end of our code, which displayed the top-performing students.\n",
    "- The Shuffle Read/Write columns show data movement during our GROUP BY and ORDER BY operations where we calculated average grades and attendance.\n",
    "- The varying durations (42ms to 3s) show which parts of our query required more processing time.\n",
    "\n",
    "### Notice also the skipped stages:\n",
    "These stages were planned for our code but optimized away thanks to our caching strategy with\n",
    " grades.cache().\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Jobs Tab Description](res/stages_2new.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
